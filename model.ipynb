{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_facenet import FaceNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from ultralytics import YOLO \n",
    "import joblib\n",
    "\n",
    "face_detector = YOLO(\"face_yolov8s.pt\")\n",
    "facenet_model = FaceNet()\n",
    "classes = [\"Aahad\", \"Ali\", \"Baba\", \"Bisma\", \"Damysha\", \"Daud\", \"Fahad\", \"Gujjar\", \"Sameed\", \"Saqib\", \"Unaiza\", \"Zunaira\"]\n",
    "\n",
    "# Load the saved classifier\n",
    "classifier = joblib.load('classifier.pkl')\n",
    "print(\"Classifier loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_faces(image_path):\n",
    "    \"\"\"\n",
    "    Detects and crops multiple face regions from an image using YOLOv8 face detection.\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "    Returns:\n",
    "        List of PIL.Image: List of cropped face images or an empty list if no face is detected.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    results = face_detector.predict(source=np.array(image), conf=0.5, iou=0.45, save=False, save_crop=False, device=\"cpu\")\n",
    "    \n",
    "    # Initialize list to hold cropped faces\n",
    "    cropped_faces = []\n",
    "    \n",
    "    # Process detection results\n",
    "    if len(results[0].boxes) > 0:\n",
    "        for box in results[0].boxes:\n",
    "            # Extract bounding box coordinates\n",
    "            coordinates = box.xyxy[0].cpu().numpy()\n",
    "            x1, y1, x2, y2 = map(int, coordinates)\n",
    "            # Crop the face and resize for FaceNet\n",
    "            cropped_face = image.crop((x1, y1, x2, y2))\n",
    "            cropped_faces.append(cropped_face.resize((160, 160)))  # Resize for FaceNet\n",
    "    \n",
    "    return cropped_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict_images(image_path):\n",
    "    \"\"\"\n",
    "    Predicts whether the faces in the input image are 'aahad' or 'sameed'.\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "    Returns:\n",
    "        List of str: Predicted class labels for each face, or an empty list if no faces are detected.\n",
    "    \"\"\"\n",
    "    # Detect and crop multiple faces\n",
    "    cropped_faces = detect_and_crop_faces(image_path)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # If no faces are detected, return a message\n",
    "    if len(cropped_faces) == 0:\n",
    "        return [\"No face detected\"]\n",
    "    \n",
    "    # Iterate over each cropped face\n",
    "    for cropped_face in cropped_faces:\n",
    "        # Convert PIL image to NumPy array\n",
    "        cropped_face_array = np.array(cropped_face)\n",
    "        \n",
    "        # Resize image to FaceNet's expected input size (160x160)\n",
    "        cropped_face_resized = cv2.resize(cropped_face_array, (160, 160))\n",
    "        \n",
    "        # Generate embedding for the cropped face\n",
    "        embedding = facenet_model.embeddings([cropped_face_resized])[0].reshape(1, -1)\n",
    "        \n",
    "        # Predict using the trained SVM classifier\n",
    "        prediction = classifier.predict(embedding)\n",
    "        predictions.append(classes[prediction[0]])  # Append the predicted class label\n",
    "    \n",
    "    return predictions  # Returns a list of predicted class labels for all faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x480 2 faces, 156.4ms\n",
      "Speed: 4.1ms preprocess, 156.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predicted class for the input image: ['Gujjar', 'Daud']\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the prediction function:\n",
    "input_image_path = \"./test/11.jpg\"\n",
    "predicted_class = predict_images(input_image_path)\n",
    "print(f\"Predicted class for the input image: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
